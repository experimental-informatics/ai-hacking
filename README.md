# AI Hacking
Georg Trogemann, Ting Chun Liu, Christian Heck
<pre>
Compact seminar
23.01. – 27.01.2023
Filzengraben 8 – 10, 0.2 [ ] ground zero, Academy of Media Arts Cologne
</pre>
Using artistic-experimental codes specifically for one’s own strategy and against the functioning of state-of-the-art AI models is called an Adversarial Attack. In Adversarial Attacks, one hacks artificial neural networks in order to reveal their modes of operation and to generate new aesthetic experiences. The goal is to discuss the cultural consequences of these technologies and to test new approaches for a changed consciousness in dealing with technology.

The first research paper on adversarial attacks was about targeted perturbations of neural computer vision systems. In 2014, Szegedy et al. introduced what they called an „intriguing property of neural networks.“ Adding a noise layer to images could trick neural image classification systems into misclassification, while the image perturbation was imperceptible to humans.

The seminar is open to anyone who already has programming experience (in any language). We will work exclusively with the Python programming language in the seminar. Programming in Python can be acquired in the basic seminar „Plunging into Code“, which takes place weekly in the WS.

The goal of the seminar is to learn how to navigate through the technical frameworks in a practical way, so that in the end each student not only knows the tools and methods of Adversarial Hacking, but is also able to develop projects independently.

